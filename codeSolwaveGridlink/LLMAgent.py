from dotenv import load_dotenv
import os
import sys
import glob
import re
import json
import random
import time
from tqdm import tqdm
from datetime import datetime
import openai
from openai import OpenAI
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM
import torch

load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    sys.exit("Missing OPENAI_API_KEY. Put it in your .env as OPENAI_API_KEY=...")

client = OpenAI()

try:
    from vllm import LLM, SamplingParams
    import ray
except ImportError:
    pass

vllm_alias = {
    'mistral': 'mistralai/Mistral-7B-v0.3',
    'mixtral': 'mistralai/Mixtral-8x7B-v0.1',
    'mistral-instruct': 'mistralai/Mistral-7B-Instruct-v0.3',
    'mixtral-instruct': 'mistralai/Mixtral-8x7B-Instruct-v0.1',

    'gemma': 'google/gemma-7b',
    'gemma-2-2b': 'google/gemma-2-2b',
    'gemma-2-2b-it': 'google/gemma-2b-it',
    'gemma-2-27b': 'google/gemma-2-27b',
    'gemma-2-27b-it': 'google/gemma-2-27b-it', # instruction tuned

    'Llama-3-70B': 'meta-llama/Meta-Llama-3-70B',
    'Llama-3-8B': 'meta-llama/Meta-Llama-3-8B',
    'Llama-3.1-405B': 'meta-llama/Meta-Llama-3.1-405B',
    'Llama-3.1-8B': 'meta-llama/Meta-Llama-3.1-8B',
    'Llama-3.1-70B': 'meta-llama/Meta-Llama-3.1-70B',
    'Llama-3.1-405B-Instruct': 'meta-llama/Meta-Llama-3.1-405B-Instruct',
    'Llama-3.1-70B-Instruct': 'meta-llama/Meta-Llama-3.1-70B-Instruct',
    'Llama-3.3-70B-Instruct': 'meta-llama/Meta-Llama-3.3-70B-Instruct',
    'Llama-3.1-8B-Instruct': 'meta-llama/Meta-Llama-3.1-8B-Instruct',

    'Qwen3-0.6B': 'Qwen/Qwen3-0.6B',
    'Qwen3-1.7B': 'Qwen/Qwen3-1.7B',
    'Qwen3-4B': 'Qwen/Qwen3-4B',
    'Qwen3-8B': 'Qwen/Qwen3-8B',
    'Qwen3-14B': 'Qwen/Qwen3-14B',
    'Qwen3-30B': 'Qwen/Qwen3-30B-A3B',
    'Qwen3-32B': 'Qwen/Qwen3-32B',
    'Qwen3-235B': 'Qwen/Qwen3-235B-A22B',
    'phi-3.5-mini-instruct': 'microsoft/phi-3.5-mini-instruct'
}

# global llm and tokenizer store to avoid reloading models
llms = {}
tokenizers = {}

class LLMAgent:
    def __init__(self, model_name: str, config: dict):
        self.model_name = model_name
        self.config = config

    def _setup_vllm(self, model: str):
        if self.config['gpus'] > 1:
            ray.init(ignore_reinit_error=True)

        print("using model ", model)
        if model in llms:
            print("Info: Model already loaded in llms, skipping setup")
            return

        # tokenizer setup
        if model in vllm_alias:
            tokenizers[model] = AutoTokenizer.from_pretrained(vllm_alias[model])
            print('Using tokenizer for', vllm_alias[model])
        else:
            print('Attempting to use tokenizer for', model)
            try:
                tokenizers[model] = AutoTokenizer.from_pretrained(model)
            except Exception:
                print('Info: passing tokenizer setup')

        # load vllm
        if model in vllm_alias:
            if "fp8" in self.config.keys() and self.config['fp8']:
                print("Using fp8")
                if vllm_alias[model] in ['meta-llama/Meta-Llama-3.1-70B', 'meta-llama/Meta-Llama-3.1-70B-Instruct']:
                    print("Set up Llama-3.1-70B-Instruct with context length of 8192")
                    llms[model] = LLM(model=vllm_alias[model], tensor_parallel_size=self.config['gpus'], download_dir=self.config['model_dir'], gpu_memory_utilization=0.65, max_model_len=8192)
                else:
                    llms[model] = LLM(model=vllm_alias[model], tensor_parallel_size=self.config['gpus'], download_dir=self.config['model_dir'], gpu_memory_utilization=0.75)
            elif vllm_alias[model] in ['meta-llama/Meta-Llama-3.1-70B', 'meta-llama/Meta-Llama-3.1-70B-Instruct']:
                llms[model] = LLM(model=vllm_alias[model], tensor_parallel_size=self.config['gpus'], download_dir=self.config['model_dir'], gpu_memory_utilization=0.95, max_model_len=12880)
            else:
                llms[model] = LLM(model=vllm_alias[model], tensor_parallel_size=self.config['gpus'], download_dir=self.config['model_dir'])
        elif model and model[0] == '/':
            # load local model
            print("Info: using finetuned model setup")
            llms[model] = AutoModelForCausalLM.from_pretrained(
                model,
                torch_dtype=torch.bfloat16,
                low_cpu_mem_usage=True,
                device_map="auto"
            )
        else:
            try:
                llms[model] = LLM(model=model, tensor_parallel_size=self.config['gpus'], download_dir=self.config['model_dir'])
            except Exception:
                print('Info: Passing vllm setup')

    def completion_create_helper(self, prompt, batch_inference=False, max_tokens=256):
        model_name = self.model_name
        config = self.config

        if "HUMAN" in model_name:
            if isinstance(prompt, list):
                responses = []
                for p in prompt:
                    print("\n", p)
                    if "NO_CONFIRM" not in model_name: # confirm just in case?
                        input("Press Enter when ready to respond...\n")
                    response = input("Your response: ")
                    responses.append(response)
                return responses
            else:
                print("\n", prompt)
                if "NO_CONFIRM" not in model_name:
                    input("Press Enter when ready to respond...\n")
                return input("Your response: ")

        if batch_inference:
            prompt = [prompt] if not isinstance(prompt, list) else prompt

        if (model_name in vllm_alias and model_name not in llms) or (model_name[0] == '/' and model_name not in llms):
            self._setup_vllm(model_name)

        tokenizer = tokenizers.get(model_name, None)
        ret = '' # return the output ret at the end and use to calculate cost

        if model_name == "gpt-3.5-turbo-instruct":
            ret = client.completions.create(
                    model="gpt-3.5-turbo-instruct",
                    prompt=prompt,
                    temperature=0.8,
                    max_tokens=max_tokens,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0
            )
            ret = ret.choices[0].text.strip()
        elif model_name in ["gpt-3.5-turbo", "gpt-3.5-turbo-16k", "gpt-4-turbo", "gpt-4o", "gpt-4o-mini"]:
            ret = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "system", "content": prompt}],
                max_tokens=max_tokens
            )
            ret = ret.choices[-1].message.content
        elif model_name in ["o4-mini"]:
            response = client.responses.create(
                model=model_name,
                reasoning={"effort": "medium"},
                input=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                    ])
            ret = response.output_text
        elif model_name in ["gpt-5-nano", "gpt-5-mini"]:
            response = client.responses.create(
                model=model_name,
                reasoning={"effort": "low"},
                input=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                    ])
            ret = response.output_text
        elif model_name in vllm_alias and model_name in llms:
            sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=max_tokens)

            if batch_inference:
                if isinstance(prompt, list):
                    messages = [{"role": "user", "content": p} for p in prompt]
                else:
                    messages = [{"role": "user", "content": prompt}]

                formatted_prompts = []

                if "Qwen3" in model_name:
                    formatted_prompts = [tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True, enable_thinking=config['thinking']) for message in messages]
                else:
                    formatted_prompts = [tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True) for message in messages]

                outputs = llms[model_name].generate(formatted_prompts, sampling_params)

                if "Qwen3" in model_name and config['thinking']:
                    ret = [output.outputs[0].text.split("</think>")[-1] for output in outputs]
                else:
                    ret = [output.outputs[0].text for output in outputs]
                if config['verbose']:
                    print("Response from model: ", ret)

            else:
                messages = [{"role": "user", "content": prompt}]
                if "Qwen3" in model_name:
                    prompt_fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=config['thinking'])
                else:
                    prompt_fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

                print("~~~~~~~~~~~~~~~~~~~~~~~~~")
                print(prompt_fmt)
                print("~~~~~~~~~~~~~~~~~~~~~~~~~")
                output = llms[model_name].generate([prompt_fmt], sampling_params)

                if 'thinking' in config and config['thinking']:
                    ret = output[0].outputs[0].text.split("</think>\n\n")[-1]
                else:
                    ret = output[0].outputs[0].text

                if config['verbose']:
                    print("Response from model: ", output[0].outputs[0].text)

        elif model_name == "phi-3.5-mini-instruct":
            model_name_hf = "microsoft/phi-3.5-mini-instruct"
            tokenizer = AutoTokenizer.from_pretrained(model_name_hf, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(model_name_hf, torch_dtype=torch.float16, device_map="auto")
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            output = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)
            ret = tokenizer.decode(output[0], skip_special_tokens=True)

        else:
            if isinstance(prompt, list):
                ret = []
                for p in prompt:
                    inputs = tokenizer(p, return_tensors='pt').to('cuda')
                    with torch.no_grad():
                        output_ids = llms[model_name].generate(**inputs, max_new_tokens=max_tokens)
                    output_ids = output_ids[:, inputs.input_ids.shape[1]:]
                    ret.append(tokenizer.decode(output_ids[0], skip_special_tokens=True))
            else:
                if 'Qwen3' in model_name:
                    print("loaded Qwen3 tokenizer")
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                messages = [{"role": "user", "content": prompt}]
                if 'Qwen3' in model_name:
                    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=config['thinking'])
                    print("||||||||||||||||||||||||||||||")
                    print(inputs)
                    print("||||||||||||||||||||||||||||||")
                else:
                    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                inputs = tokenizer(inputs, return_tensors='pt').to('cuda')
                with torch.no_grad():
                    output_ids = llms[model_name].generate(**inputs, max_new_tokens=max_tokens)
                output_ids = output_ids[:, inputs.input_ids.shape[1]:]
                ret = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                print("~~~~~~~~~~~~~~~~~~~~~~~~~")
                print("Generated output: ", ret)
                print("~~~~~~~~~~~~~~~~~~~~~~~~~")
                if '</think>' in ret:
                    ret = ret.split("</think>")[-1]
                if '<im_end>' in ret:
                    ret = ret.split("<im_end>")[0]

        if config['verbose']:
            print("Response from model: ", ret)
        return ret

    def completion_create(self, prompt, max_tokens=256, keep_trying=False, batch_inference=False):
        try:
            return self.completion_create_helper(prompt, batch_inference, max_tokens)
        except (openai.APIError, openai.OpenAIError) as e:
            print(e)
            time.sleep(10)
            if keep_trying:
                return self.completion_create(prompt, max_tokens, keep_trying, batch_inference)
            else:
                return None

    def __call__(self, prompt, max_tokens=256, keep_trying=False, batch_inference=False):
        return self.completion_create(prompt, max_tokens, keep_trying, batch_inference)

    def __repr__(self):
        return f"LLM: {self.model_name} (Config: {self.config})"



